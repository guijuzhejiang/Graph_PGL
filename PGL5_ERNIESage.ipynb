{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ERNIESage代码解析\n",
    "\n",
    "本项目主要是为了直接提供一个可以运行ERNIESage模型的代码介绍，以便同学们能够直观感受到ERNIESage的魅力，同时也会对ERNIESage中的部分关键代码进行必要讲解。Let's enjoy!\n",
    "\n",
    "**ERNIESage**可以很轻松地在PGL中的消息传递范式中进行实现，目前PGL在github上提供了3个版本的ERNIESage模型：\n",
    "- **ERNIESage v1**: ERNIE 作用于text graph节点上;\n",
    "- **ERNIESage v2**: ERNIE 作用在text graph的边上;\n",
    "- **ERNIESage v3**: ERNIE 作用于一阶邻居及起边上;\n",
    "\n",
    "### 讲解流程\n",
    "- 数据\n",
    "- **模型**\n",
    "- 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 拉取PGL代码，由于github拉取较慢，已经提前拉取完毕了\n",
    "# !git clone https://github.com/PaddlePaddle/PGL\n",
    "# !cd PGL/example/erniesage\n",
    "# 为了正常运行代码，首先我们需要安装以下依赖\n",
    "!pip install pgl\n",
    "!pip install easydict\n",
    "!python3 -m pip install --no-deps paddle-propeller\n",
    "!pip install paddle-ernie\n",
    "!pip uninstall -y colorlog\n",
    "!export CUDAV_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据\n",
    "### 输入example数据集\n",
    "1. example_data/link_predict/graph_data.txt - 简单的输入文件，格式为每行query \\t answer，可作简单的运行实例使用，link predict任务一般直接用图中的边作为训练目标。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head: 无法打开'example_data/link_predict/graph_data.txt' 读取数据: 没有那个文件或目录\n",
      "wc: example_data/link_predict/graph_data.txt: 没有那个文件或目录\n"
     ]
    }
   ],
   "source": [
    "! head -n 3 example_data/link_predict/graph_data.txt\n",
    "! wc -l example_data/link_predict/graph_data.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 如何表达一个文本图\n",
    "- 出现过的每一个文本段当作一个节点，比如“黑缘粗角肖叶甲触角有多大？”就是一个节点\n",
    "- 一行两个节点作为一条边\n",
    "- 节点的文本段逐字转成id，形成id序列，作为**节点特征**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing.dump_graph import dump_graph\n",
    "from preprocessing.dump_graph import dump_node_feat\n",
    "from preprocessing.dump_graph import download_ernie_model\n",
    "from preprocessing.dump_graph import load_config\n",
    "from pgl.graph_wrapper import BatchGraphWrapper\n",
    "import propeller.paddle as propeller\n",
    "import paddle.fluid as F\n",
    "import paddle.fluid.layers as L\n",
    "import numpy as np\n",
    "from preprocessing.dump_graph import load_config\n",
    "from models.pretrain_model_loader import PretrainedModelLoader\n",
    "from pgl.graph import MemmapGraph\n",
    "from models.encoder import linear\n",
    "from ernie import ErnieModel\n",
    "np.random.seed(123)\n",
    "config = load_config(\"./config/erniesage_link_predict.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将原始QA数据产出一个文本图，并使用grpah.dump存放到 workdir 目录下\n",
    "dump_graph(config)\n",
    "dump_node_feat(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MemmapGraph可以将PGL中graph.dump的模型，重新load回来\n",
    "graph = MemmapGraph(\"./workdir/\") \n",
    "# 看一下图基础信息\n",
    "print(\"节点\", graph.num_nodes,\"个\") \n",
    "print(\"边\", graph.edges, graph.edges.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 看一下节点特征\n",
    "print([(\"%s shape is %s\" % (key, str(graph.node_feat[key].shape))) for key in graph.node_feat])\n",
    "print(graph.node_feat) #  按字的粒度转成ID，每段文本为一个节点，文本全部保留40长度\n",
    "# 1021个节点，每个节点有长度为40的id序列"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型\n",
    "### ERNIESage V1 模型核心流程\n",
    "- ERNIE提取节点语义 -> GNN聚合\n",
    "\n",
    "<img src=\"https://ai-studio-static-online.cdn.bcebos.com/0ab25a4f0c1647acbcfacc1be2066d47e98ec4f1931d4dcebd209347dc1b5448\" alt=\"图片替换文本\" width=\"300\" height=\"313\" align=\"bottom\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ERNIESage V1，ERNIE作用在节点上\n",
    "class ERNIESageV1Encoder():\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "\n",
    "    def __call__(self, graph_wrappers, inputs):\n",
    "        \n",
    "        # step1. ERNIE提取节点语义\n",
    "        # 输入每个节点的文本的id序列\n",
    "        term_ids = graph_wrappers[0].node_feat[\"term_ids\"]\n",
    "        \n",
    "        cls = L.fill_constant_batch_size_like(term_ids, [-1, 1], \"int64\",\n",
    "                                              self.config.cls_id) # cls [B, 1]\n",
    "        term_ids = L.concat([cls, term_ids], 1) # term_ids [B, S]\n",
    "        # [CLS], id1, id2, id3 .. [SEP]\n",
    "\n",
    "        ernie_model = ErnieModel(self.config.ernie_config) \n",
    "        # 获得ERNIE的[CLS]位置的表达\n",
    "        cls_feat, _ = ernie_model(term_ids) # cls_feat [B, F]\n",
    "\n",
    "        # step2. GNN聚合\n",
    "        feature = graphsage_sum(cls_feat, graph_wrappers[0], self.config.hidden_size, \"v1_graphsage_sum\", \"leaky_relu\")\n",
    "        \n",
    "        final_feats = [\n",
    "            self.take_final_feature(feature, i, \"v1_final_fc\") for i in inputs\n",
    "        ]\n",
    "        return final_feats\n",
    "    \n",
    "    def take_final_feature(self, feature, index, name):\n",
    "        \"\"\"take final feature\"\"\"\n",
    "        feat = L.gather(feature, index, overwrite=False)\n",
    "        feat = linear(feat, self.config.hidden_size, name)\n",
    "        feat = L.l2_normalize(feat, axis=1)\n",
    "        return feat\n",
    "\n",
    "\n",
    "def graphsage_sum(feature, gw, hidden_size, name, act):\n",
    "    # copy_send\n",
    "    msg = gw.send(lambda src, dst, edge: src[\"h\"], nfeat_list=[(\"h\", feature)])\n",
    "    # sum_recv\n",
    "    neigh_feature = gw.recv(msg, lambda feat: L.sequence_pool(feat, pool_type=\"sum\"))\n",
    "\n",
    "    self_feature = linear(feature, hidden_size, name+\"_l\", act)\n",
    "    neigh_feature = linear(neigh_feature, hidden_size, name+\"_r\", act)\n",
    "    output = L.concat([self_feature, neigh_feature], axis=1) # [B, 2H]\n",
    "    output = L.l2_normalize(output, axis=1)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机构造些数据\n",
    "feat_size = 40\n",
    "feed_dict = {\n",
    "    \"num_nodes\": np.array([4]),\n",
    "    \"num_edges\": np.array([6]),\n",
    "    \"edges\": np.array([[0,1],[1,0],[0,2],[2,0],[0,3],[3,0]]),\n",
    "    \"term_ids\": np.random.randint(4, 10000, size=(4, feat_size)),\n",
    "    \"inputs\": np.array([0])}\n",
    "place = F.CUDAPlace(0)\n",
    "exe = F.Executor(place)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://ai-studio-static-online.cdn.bcebos.com/94ab49de20ec4574a5d27e7ad3d23354df5ade177666450ba2f6d4cde11c33b6\" alt=\"图片替换文本\" width=\"300\" height=\"313\" align=\"bottom\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型v1\n",
    "erniesage_v1_encoder = ERNIESageV1Encoder(config)\n",
    "\n",
    "main_prog, start_prog = F.Program(), F.Program()\n",
    "with F.program_guard(main_prog, start_prog):\n",
    "    with F.unique_name.guard():\n",
    "        num_nodes = L.data(\"num_nodes\", [1], False, 'int64')\n",
    "        num_edges = L.data(\"num_edges\", [1], False, 'int64')\n",
    "        edges = L.data(\"edges\", [-1, 2], False, 'int64')\n",
    "        node_feat = L.data(\"term_ids\", [-1, 40], False, 'int64')\n",
    "        inputs = L.data(\"inputs\", [-1], False, 'int64')\n",
    "\n",
    "        # 输入图的基本信息（边、点、特征）构造一个graph \n",
    "        gw = BatchGraphWrapper(num_nodes, num_edges, edges, {\"term_ids\": node_feat})\n",
    "        outputs = erniesage_v1_encoder([gw], [inputs])\n",
    "\n",
    "exe.run(start_prog)\n",
    "outputs_np = exe.run(main_prog, feed=feed_dict, fetch_list=[outputs])[0]\n",
    "print(outputs_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ERNIESage V2 核心代码\n",
    "- GNN send 文本id -> ERNIE提取边语义 -> GNN recv 聚合邻居语义 -> ERNIE提取中心节点语义并concat\n",
    "\n",
    "<img src=\"https://ai-studio-static-online.cdn.bcebos.com/24d5cca257624cc6bb94eeea7a7c3f84512534070c5949a5a9aca8fc8455f52e\" alt=\"图片替换文本\" width=\"500\" height=\"313\" align=\"bottom\" />\n",
    "\n",
    "为了使得大家对下面有关ERNIE模型的部分能够有所了解，这里先贴出ERNIE的主模型框架图。\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/8b2bf7e82042474e904e867b415b83fed436281fe75e46dca1f9cb97189172bc)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ERNIESage V2，ERNIE作用在边上\n",
    "class ERNIESageV2Encoder():\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "\n",
    "    def __call__(self, graph_wrappers, inputs):\n",
    "        gw = graph_wrappers[0]\n",
    "        term_ids = gw.node_feat[\"term_ids\"] # term_ids [B, S]\n",
    "        \n",
    "        # step1. GNN send 文本id\n",
    "        def ernie_send(src_feat, dst_feat, edge_feat):\n",
    "            def build_position_ids(term_ids):\n",
    "                input_mask = L.cast(term_ids > 0, \"int64\")\n",
    "                position_ids = L.cumsum(input_mask, axis=1) - 1\n",
    "                return position_ids\n",
    "            \n",
    "            # src_ids, dst_ids 为发送src和接收dst节点分别的文本ID序列\n",
    "            src_ids, dst_ids = src_feat[\"term_ids\"], dst_feat[\"term_ids\"]\n",
    "\n",
    "            # 生成[CLS]对应的id列, 并与前半段concat\n",
    "            cls = L.fill_constant_batch_size_like(\n",
    "                src_feat[\"term_ids\"], [-1, 1], \"int64\", self.config.cls_id) # cls [B, 1]\n",
    "            src_ids = L.concat([cls, src_ids], 1) # src_ids [B, S+1]\n",
    "\n",
    "            # 将src与dst concat在一起作为完整token ids\n",
    "            term_ids = L.concat([src_ids, dst_ids], 1) # term_ids [B, 2S+1]\n",
    "            # [CLS], src_id1, src_id2.. [SEP], dst_id1, dst_id2..[SEP]\n",
    "\n",
    "            sent_ids = L.concat([L.zeros_like(src_ids), L.ones_like(dst_ids)], 1)\n",
    "            #   0, 0, 0 .. 0, 1, 1 .. 1 \n",
    "\n",
    "            position_ids = build_position_ids(term_ids)\n",
    "            #   0, 1, 2, 3 ..  \n",
    "            \n",
    "            # step2. ERNIE提取边语义 \n",
    "            ernie_model = ErnieModel(self.config.ernie_config)\n",
    "            cls_feat, _ = ernie_model(term_ids, sent_ids, position_ids)\n",
    "            # cls_feat 为ERNIE提取的句子级隐向量表达\n",
    "            return cls_feat\n",
    "\n",
    "        msg = gw.send(ernie_send, nfeat_list=[(\"term_ids\", term_ids)])\n",
    "        \n",
    "        # step3. GNN recv 聚合邻居语义 \n",
    "        # 接收了邻居的CLS语义表达，sum聚合在一起\n",
    "        neigh_feature = gw.recv(msg, lambda feat: F.layers.sequence_pool(feat, pool_type=\"sum\"))\n",
    "\n",
    "        # 为每个节点也拼接一个CLS表达\n",
    "        cls = L.fill_constant_batch_size_like(term_ids, [-1, 1],\n",
    "                                              \"int64\", self.config.cls_id)\n",
    "        \n",
    "        term_ids = L.concat([cls, term_ids], 1)\n",
    "        # [CLS], id1, id2, ... [SEP]\n",
    "        \n",
    "        # step4. ERNIE提取中心节点语义并concat\n",
    "        # 对中心节点过一次ERNIE    \n",
    "        ernie_model = ErnieModel(self.config.ernie_config)\n",
    "\n",
    "        # 获取中心节点的语义CLS表达\n",
    "        self_cls_feat, _ = ernie_model(term_ids)\n",
    "\n",
    "        hidden_size = self.config.hidden_size        \n",
    "        self_feature = linear(self_cls_feat, hidden_size, \"erniesage_v2_l\", \"leaky_relu\")\n",
    "        neigh_feature = linear(neigh_feature, hidden_size, \"erniesage_v2_r\", \"leaky_relu\")\n",
    "        output = L.concat([self_feature, neigh_feature], axis=1)\n",
    "        output = L.l2_normalize(output, axis=1)\n",
    "\n",
    "        final_feats = [\n",
    "            self.take_final_feature(output, i, \"v2_final_fc\") for i in inputs\n",
    "        ]\n",
    "        return final_feats\n",
    "\n",
    "    def take_final_feature(self, feature, index, name):\n",
    "        \"\"\"take final feature\"\"\"\n",
    "        feat = L.gather(feature, index, overwrite=False)\n",
    "        feat = linear(feat, self.config.hidden_size, name)\n",
    "        feat = L.l2_normalize(feat, axis=1)\n",
    "        return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 直接run一下\n",
    "erniesage_v2_encoder = ERNIESageV2Encoder(config)\n",
    "\n",
    "main_prog, start_prog = F.Program(), F.Program()\n",
    "with F.program_guard(main_prog, start_prog):\n",
    "    with F.unique_name.guard():\n",
    "        num_nodes = L.data(\"num_nodes\", [1], False, 'int64')\n",
    "        num_edges = L.data(\"num_edges\", [1], False, 'int64')\n",
    "        edges = L.data(\"edges\", [-1, 2], False, 'int64')\n",
    "        node_feat = L.data(\"term_ids\", [10, 40], False, 'int64')\n",
    "        inputs = L.data(\"inputs\", [2], False, 'int64')\n",
    "\n",
    "        gw = BatchGraphWrapper(num_nodes, num_edges, edges, {\"term_ids\": node_feat})\n",
    "        outputs = erniesage_v2_encoder([gw], [inputs])\n",
    "\n",
    "exe = F.Executor(place)\n",
    "exe.run(start_prog)\n",
    "outputs_np = exe.run(main_prog, feed=feed_dict, fetch_list=[outputs])[0]\n",
    "print(outputs_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ERNIESage V3 核心过程\n",
    "- GNN send 文本id序列 -> GNN recv 拼接文本id序列 -> ERNIE同时提取中心和多个邻居语义表达\n",
    "<img src=\"https://ai-studio-static-online.cdn.bcebos.com/b18ab78738764e88b624d1db8ce5e95c72a4161cd4b845cb80bdf0d5e914cfbc\" alt=\"图片替换文本\" width=\"500\" height=\"313\" align=\"bottom\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.encoder import v3_build_sentence_ids\n",
    "from models.encoder import v3_build_position_ids\n",
    "\n",
    "class ERNIESageV3Encoder():\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "\n",
    "    def __call__(self, graph_wrappers, inputs):\n",
    "        gw = graph_wrappers[0]\n",
    "        term_ids = gw.node_feat[\"term_ids\"]\n",
    "\n",
    "        # step1. GNN send 文本id序列\n",
    "        # copy_send\n",
    "        msg = gw.send(lambda src, dst, edge: src[\"h\"], nfeat_list=[(\"h\", term_ids)])\n",
    "\n",
    "        # step2. GNN recv 拼接文本id序列\n",
    "        def ernie_recv(term_ids):\n",
    "            \"\"\"doc\"\"\"\n",
    "            num_neighbor = self.config.samples[0]\n",
    "            pad_value = L.zeros([1], \"int64\")\n",
    "\n",
    "            # 这里使用seq_pad，将num_neighbor个邻居节点的文本id序列拼接在一下\n",
    "            # 对于不足num_neighbor个邻居的将会pad到num_neighbor个\n",
    "            neighbors_term_ids, _ = L.sequence_pad(\n",
    "                term_ids, pad_value=pad_value, maxlen=num_neighbor) # [B, N*S]\n",
    "\n",
    "            neighbors_term_ids = L.reshape(neighbors_term_ids, [0, self.config.max_seqlen * num_neighbor])\n",
    "            return neighbors_term_ids\n",
    "    \n",
    "        neigh_term_ids = gw.recv(msg, ernie_recv)\n",
    "        neigh_term_ids = L.cast(neigh_term_ids, \"int64\")\n",
    "\n",
    "        # step3. ERNIE同时提取中心和多个邻居语义表达\n",
    "        cls = L.fill_constant_batch_size_like(term_ids, [-1, 1], \"int64\",\n",
    "                                              self.config.cls_id) # [B, 1]\n",
    "\n",
    "        # 将中心与多个邻居的文本全部拼接在一起，形成超长的文本（num_nerghbor+1) * seqlen\n",
    "        multi_term_ids = L.concat([cls, term_ids[:, :-1], neigh_term_ids], 1) # multi_term_ids [B, (N+1)*S]\n",
    "        # [CLS], center_id1, center_id2..[SEP]n1_id1, n1_id2..[SEP]n2_id1, n2_id2..[SEP]..[SEP]\n",
    "        slot_seqlen = self.config.max_seqlen\n",
    "        final_feats = []\n",
    "        for index in inputs:\n",
    "            term_ids = L.gather(multi_term_ids, index, overwrite=False)\n",
    "            position_ids = v3_build_position_ids(term_ids, slot_seqlen)\n",
    "            sent_ids = v3_build_sentence_ids(term_ids, slot_seqlen)\n",
    "\n",
    "            # 将需要计算的超长文本，使用Ernie提取CLS位置的语义表达\n",
    "            ernie_model = ErnieModel(self.config.ernie_config)\n",
    "            cls_feat, _ = ernie_model(term_ids, sent_ids, position_ids)\n",
    "\n",
    "            feature = linear(cls_feat, self.config.hidden_size, \"v3_final_fc\")\n",
    "            feature = L.l2_normalize(feature, axis=1)\n",
    "            final_feats.append(feature)\n",
    "        return final_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 直接run一下\n",
    "erniesage_v3_encoder = ERNIESageV3Encoder(config)\n",
    "\n",
    "main_prog, start_prog = F.Program(), F.Program()\n",
    "with F.program_guard(main_prog, start_prog):\n",
    "    num_nodes = L.data(\"num_nodes\", [1], False, 'int64')\n",
    "    num_edges = L.data(\"num_edges\", [1], False, 'int64')\n",
    "    edges = L.data(\"edges\", [-1, 2], False, 'int64')\n",
    "    node_feat = L.data(\"term_ids\", [-1, 40], False, 'int64')\n",
    "    inputs = L.data(\"inputs\", [-1], False, 'int64')\n",
    "\n",
    "    gw = BatchGraphWrapper(num_nodes, num_edges, edges, {\"term_ids\": node_feat})\n",
    "    outputs = erniesage_v3_encoder([gw], [inputs])\n",
    "\n",
    "exe.run(start_prog)\n",
    "outputs_np = exe.run(main_prog, feed=feed_dict, fetch_list=[outputs])[0]\n",
    "print(outputs_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练\n",
    "### link predict任务\n",
    "以一个link predict的任务为例，读取一个语义图，以上面的边为目标进行无监督的训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ERNIESageLinkPredictModel(propeller.train.Model):\n",
    "    def __init__(self, hparam, mode, run_config):\n",
    "        self.hparam = hparam\n",
    "        self.mode = mode\n",
    "        self.run_config = run_config\n",
    "\n",
    "    def forward(self, features):\n",
    "        num_nodes, num_edges, edges, node_feat_index, node_feat_term_ids, user_index, \\\n",
    "            pos_item_index, neg_item_index, user_real_index, pos_item_real_index = features\n",
    "\n",
    "        node_feat = {\"index\": node_feat_index, \"term_ids\": node_feat_term_ids}\n",
    "        graph_wrapper = BatchGraphWrapper(num_nodes, num_edges, edges,\n",
    "                                          node_feat)\n",
    "\n",
    "        #encoder = ERNIESageV1Encoder(self.hparam)\n",
    "        encoder = ERNIESageV2Encoder(self.hparam)\n",
    "        #encoder = ERNIESageV3Encoder(self.hparam)\n",
    "\n",
    "        # 中心节点、邻居节点、随机采样节点 分别提取特征\n",
    "        outputs = encoder([graph_wrapper],\n",
    "                          [user_index, pos_item_index, neg_item_index])\n",
    "        user_feat, pos_item_feat, neg_item_feat = outputs\n",
    "    \n",
    "        if self.mode is not propeller.RunMode.PREDICT:\n",
    "            return user_feat, pos_item_feat, neg_item_feat\n",
    "        else:\n",
    "            return user_feat, user_real_index\n",
    "\n",
    "    def loss(self, predictions, labels):\n",
    "        user_feat, pos_item_feat, neg_item_feat = predictions\n",
    "        pos = L.reduce_sum(user_feat * pos_item_feat, -1, keep_dim=True) # \n",
    "        #neg = L.reduce_sum(user_feat * neg_item_feat, -1, keep_dim=True)# 60.\n",
    "        neg = L.matmul(user_feat, neg_item_feat, transpose_y=True) # 80.\n",
    "        # 距离（中心，邻居）> 距离(中心，随机负)\n",
    "        loss = L.reduce_mean(L.relu(neg - pos + self.hparam.margin))\n",
    "        return loss\n",
    "\n",
    "    def backward(self, loss):\n",
    "        adam = F.optimizer.Adam(learning_rate=self.hparam['learning_rate'])\n",
    "        adam.minimize(loss)\n",
    "\n",
    "    def metrics(self, predictions, label):\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from link_predict import train\n",
    "from link_predict import predict\n",
    "\n",
    "train(config, ERNIESageLinkPredictModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(config, ERNIESageLinkPredictModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head output/part-0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 如何评价\n",
    "\n",
    "为了可以比较清楚地知道Embedding的效果，我们直接通过MRR简单判断一下graphp_data.txt计算出来的Embedding结果，此处将graph_data.txt同时作为训练集和验证集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python build_dev.py --path \"./example_data/link_predict/graph_data.txt\" # 此命令用于将训练数据输出为需要的格式，产生的文件为dev_out.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 接下来，计算MRR得分。\n",
    "# 注意，运行此代码的前提是，我们已经将config对应的yaml配置文件中的input_data参数修改为了：\"data.txt\"\n",
    "# 并且注意训练的模型是针对data.txt的，如果不符合，请重新训练模型。\n",
    "!python mrr.py --emb_path output/part-0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "通过以上三个版本的模型代码简单的讲解，我们可以知道他们的不同点，其实主要就是在消息传递机制的部分有所不同。ERNIESageV1版本只作用在text graph的节点上，在传递消息(Send阶段)时只考虑了邻居本身的文本信息；而ERNIESageV2版本则作用在了边上，在Send阶段同时考虑了当前节点和其邻居节点的文本信息，达到更好的交互效果，\n",
    "ERNIESageV3则作用在中心和全部邻居上，使节点之间能够互相attention。\n",
    "\n",
    "希望通过这一运行实例，可以帮助同学们对ERNIESage有更好的了解和认识，大家快快用起来吧！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}